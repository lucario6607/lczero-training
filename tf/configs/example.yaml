%YAML 1.2
--- 
name: "mamba2-8x128-multidata" # Name reflects the new, combined dataset
gpu: 0
dataset:
  num_chunks: 30000000
  allow_less_chunks: true
  train_ratio: 0.95
  sort_type: name
  fast_chunk_loading: false
  
  # --- UPDATED DATA PATHS ---
  # The training script will now pull data from all of these directories.
  input: 
    - '/notebooks/training-run1-test80-20220404-0817/'
    - '/notebooks/training-run1-test80-20240129-1017/'
    - '/notebooks/training-run1-test80-20240208-1417/'
    - '/notebooks/training-run1-test80-20240214-0217/'
    - '/notebooks/training-run1-test80-20250620-0017/'
    - '/notebooks/training-run1-test80-20250620-0117/'
    - '/notebooks/training-run1-test80-20250620-0317/'
  # --- END OF UPDATED PATHS ---

training:
    # Using bfloat16 for the best balance of stability and memory efficiency
    precision: bfloat16 
    
    batch_size: 4096
    num_batch_splits: 16
    shuffle_size: 800000 
    test_steps: 1000
    train_avg_report_steps: 1000
    checkpoint_steps: 100000
    total_steps: 3000000
    path: 'networks' 
    optimizer: nadam
    warmup_steps: 1000
    lr_values: [0.001, 0.000316, 0.0001]
    lr_boundaries: [2400000, 2700000]
    loss_weights:
        policy: 1.0
        value_winner: 1.0
        moves_left: 0.1
        reg: 0.000001

model:
    network: 'mamba2'
    
    # Model size configuration for 8 layers, 128 depth
    embedding_size: 128
    encoder_layers: 8

    # Derived parameters
    mamba2_num_heads: 8
    mamba2_head_dim: 16
    mamba2_dt_rank: 8
    
    value: 'wdl'
    moves_left: 'v1'
    input_type: 'classic'
    default_activation: 'swish'
    
    # Flags to correctly parse the data
    value_st: true
    value_q: true
    soft_policy: true
    value_q_err: false
    value_st_err: false
    categorical_value_buckets: 0

    # Mamba / RoPE settings
    rope_2d_enabled: true
    rope_2d_theta: 10000.0
    mamba2_d_state: 16
    mamba2_d_conv: 4
    mamba2_expand_factor: 2
    mamba2_chunk_size: 256
    mamba2_use_norm: true
